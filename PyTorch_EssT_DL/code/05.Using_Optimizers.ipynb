{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HBehvqZ8znsy"
   },
   "source": [
    "# Using optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bCdIqY0tKbvS"
   },
   "outputs": [],
   "source": [
    "# Setting seeds to try and ensure we have the same results - this is not guaranteed across PyTorch releases.\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PCJzXv0OK1Bs"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "mean, std = (0.5,), (0.5,)\n",
    "\n",
    "# Create a transform and normalise data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean, std)\n",
    "                              ])\n",
    "\n",
    "# Download FMNIST training dataset and load training data\n",
    "trainset = datasets.FashionMNIST('~/.pytorch/FMNIST/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download FMNIST test dataset and load test data\n",
    "testset = datasets.FashionMNIST('~/.pytorch/FMNIST/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TZpZ12MrEDZI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rqMqFbIVrbFH"
   },
   "outputs": [],
   "source": [
    "class FMNIST(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.fc1 = nn.Linear(784, 128)\n",
    "    self.fc2 = nn.Linear(128,64)\n",
    "    self.fc3 = nn.Linear(64,10)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    x = x.view(x.shape[0], -1)\n",
    "    \n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.relu(self.fc2(x))\n",
    "    x = self.fc3(x)\n",
    "    x = F.log_softmax(x, dim=1)\n",
    "    \n",
    "    return x\n",
    "    \n",
    "#model = FMNIST()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m68OeMRdEF0X"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8c0QgxCF3fD-"
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AjBut_7lhAc8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p2ZAGFzFEQA_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-iPQek2nz2yu"
   },
   "outputs": [],
   "source": [
    "images, labels = next(iter(trainloader))\n",
    "images = images.view(images.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xMnVwV-CERd_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "roihp-kN0Jw5"
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KvbHIyPSEUPh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gtP3nCEQEUMH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YwcPkxQwEfYX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Nf2WdmP5Gst"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights :  Parameter containing:\n",
      "tensor([[-0.0003,  0.0192, -0.0294,  ...,  0.0219,  0.0037,  0.0021],\n",
      "        [-0.0198, -0.0150, -0.0104,  ..., -0.0203, -0.0060, -0.0299],\n",
      "        [-0.0201,  0.0149, -0.0333,  ..., -0.0203,  0.0012,  0.0080],\n",
      "        ...,\n",
      "        [ 0.0018, -0.0295,  0.0085,  ..., -0.0037,  0.0036,  0.0300],\n",
      "        [-0.0233, -0.0220, -0.0064,  ...,  0.0115, -0.0324, -0.0158],\n",
      "        [ 0.0309,  0.0066,  0.0125,  ...,  0.0286,  0.0350, -0.0105]],\n",
      "       requires_grad=True)\n",
      "Initial weights gradient :  tensor([[-7.2154e-04, -7.2154e-04, -7.2154e-04,  ..., -7.1856e-04,\n",
      "         -7.2154e-04, -7.2154e-04],\n",
      "        [ 1.5117e-03,  1.5168e-03,  1.5292e-03,  ...,  1.5526e-03,\n",
      "          1.5154e-03,  1.5168e-03],\n",
      "        [-4.5585e-04, -4.5585e-04, -4.5585e-04,  ..., -4.5585e-04,\n",
      "         -4.5585e-04, -4.5585e-04],\n",
      "        ...,\n",
      "        [-9.6583e-05, -9.6583e-05, -9.6583e-05,  ..., -1.5217e-04,\n",
      "         -9.6583e-05, -9.6583e-05],\n",
      "        [-5.6446e-04, -5.6559e-04, -5.5394e-04,  ..., -5.1035e-04,\n",
      "         -5.6446e-04, -5.6559e-04],\n",
      "        [ 3.7556e-03,  3.7518e-03,  3.7486e-03,  ...,  3.7677e-03,\n",
      "          3.7556e-03,  3.7518e-03]])\n"
     ]
    }
   ],
   "source": [
    "output = model(images)\n",
    "loss = criterion(output, labels)\n",
    "loss.backward()\n",
    "print('Initial weights : ',model[0].weight)\n",
    "print('Initial weights gradient : ',model[0].weight.grad)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "arwzAK-1EkEH"
   },
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zD-u49yzEj6v"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PuGKi_nq6P0j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights :  Parameter containing:\n",
      "tensor([[-0.0003,  0.0192, -0.0294,  ...,  0.0219,  0.0037,  0.0021],\n",
      "        [-0.0198, -0.0150, -0.0105,  ..., -0.0203, -0.0060, -0.0300],\n",
      "        [-0.0201,  0.0149, -0.0333,  ..., -0.0203,  0.0012,  0.0080],\n",
      "        ...,\n",
      "        [ 0.0018, -0.0295,  0.0085,  ..., -0.0037,  0.0036,  0.0300],\n",
      "        [-0.0232, -0.0220, -0.0064,  ...,  0.0115, -0.0324, -0.0158],\n",
      "        [ 0.0309,  0.0065,  0.0125,  ...,  0.0285,  0.0349, -0.0106]],\n",
      "       requires_grad=True)\n",
      "Initial weights gradient :  tensor([[-7.2154e-04, -7.2154e-04, -7.2154e-04,  ..., -7.1856e-04,\n",
      "         -7.2154e-04, -7.2154e-04],\n",
      "        [ 1.5117e-03,  1.5168e-03,  1.5292e-03,  ...,  1.5526e-03,\n",
      "          1.5154e-03,  1.5168e-03],\n",
      "        [-4.5585e-04, -4.5585e-04, -4.5585e-04,  ..., -4.5585e-04,\n",
      "         -4.5585e-04, -4.5585e-04],\n",
      "        ...,\n",
      "        [-9.6583e-05, -9.6583e-05, -9.6583e-05,  ..., -1.5217e-04,\n",
      "         -9.6583e-05, -9.6583e-05],\n",
      "        [-5.6446e-04, -5.6559e-04, -5.5394e-04,  ..., -5.1035e-04,\n",
      "         -5.6446e-04, -5.6559e-04],\n",
      "        [ 3.7556e-03,  3.7518e-03,  3.7486e-03,  ...,  3.7677e-03,\n",
      "          3.7556e-03,  3.7518e-03]])\n"
     ]
    }
   ],
   "source": [
    "print('Initial weights : ',model[0].weight)\n",
    "print('Initial weights gradient : ',model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O8oIy5SkEpDn"
   },
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FnfpzGigEpAr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EniqxHDwDa8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights :  Parameter containing:\n",
      "tensor([[-0.0003,  0.0192, -0.0294,  ...,  0.0219,  0.0037,  0.0021],\n",
      "        [-0.0198, -0.0150, -0.0105,  ..., -0.0203, -0.0060, -0.0300],\n",
      "        [-0.0201,  0.0149, -0.0333,  ..., -0.0203,  0.0012,  0.0080],\n",
      "        ...,\n",
      "        [ 0.0018, -0.0295,  0.0085,  ..., -0.0037,  0.0036,  0.0300],\n",
      "        [-0.0232, -0.0220, -0.0064,  ...,  0.0115, -0.0324, -0.0158],\n",
      "        [ 0.0309,  0.0065,  0.0125,  ...,  0.0285,  0.0349, -0.0106]],\n",
      "       requires_grad=True)\n",
      "Initial weights gradient :  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print('Initial weights : ',model[0].weight)\n",
    "print('Initial weights gradient : ',model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9DViAViGEwyr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mGZhQE3tDcqb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1, Loss: 2.288628339767456\n",
      "Batch: 2, Loss: 2.3080620765686035\n",
      "Batch: 3, Loss: 2.3098013401031494\n",
      "Batch: 4, Loss: 2.284043312072754\n",
      "Batch: 5, Loss: 2.291738986968994\n",
      "Batch: 6, Loss: 2.3061680793762207\n",
      "Batch: 7, Loss: 2.2870423793792725\n",
      "Batch: 8, Loss: 2.276984453201294\n",
      "Batch: 9, Loss: 2.2776780128479004\n",
      "Batch: 10, Loss: 2.3024075031280518\n",
      "Batch: 11, Loss: 2.270155906677246\n",
      "Batch: 12, Loss: 2.288559913635254\n",
      "Batch: 13, Loss: 2.2820968627929688\n",
      "Batch: 14, Loss: 2.2649171352386475\n",
      "Batch: 15, Loss: 2.274873733520508\n",
      "Batch: 16, Loss: 2.2665631771087646\n",
      "Batch: 17, Loss: 2.2486894130706787\n",
      "Batch: 18, Loss: 2.25093150138855\n",
      "Batch: 19, Loss: 2.2826104164123535\n",
      "Batch: 20, Loss: 2.2532405853271484\n",
      "Batch: 21, Loss: 2.234309434890747\n",
      "Batch: 22, Loss: 2.2593002319335938\n",
      "Batch: 23, Loss: 2.248821496963501\n",
      "Batch: 24, Loss: 2.2512760162353516\n",
      "Batch: 25, Loss: 2.257256507873535\n",
      "Batch: 26, Loss: 2.2401793003082275\n",
      "Batch: 27, Loss: 2.214656352996826\n",
      "Batch: 28, Loss: 2.226203441619873\n",
      "Batch: 29, Loss: 2.2151873111724854\n",
      "Batch: 30, Loss: 2.2176032066345215\n",
      "Batch: 31, Loss: 2.23728084564209\n",
      "Batch: 32, Loss: 2.2319467067718506\n",
      "Batch: 33, Loss: 2.2288172245025635\n",
      "Batch: 34, Loss: 2.2359671592712402\n",
      "Batch: 35, Loss: 2.2106552124023438\n",
      "Batch: 36, Loss: 2.2009663581848145\n",
      "Batch: 37, Loss: 2.228806495666504\n",
      "Batch: 38, Loss: 2.2145836353302\n",
      "Batch: 39, Loss: 2.2042295932769775\n",
      "Batch: 40, Loss: 2.1886141300201416\n",
      "Batch: 41, Loss: 2.20263409614563\n",
      "Batch: 42, Loss: 2.199158191680908\n",
      "Batch: 43, Loss: 2.169827938079834\n",
      "Batch: 44, Loss: 2.14996075630188\n",
      "Batch: 45, Loss: 2.1614444255828857\n",
      "Batch: 46, Loss: 2.1536142826080322\n",
      "Batch: 47, Loss: 2.1791305541992188\n",
      "Batch: 48, Loss: 2.174588441848755\n",
      "Batch: 49, Loss: 2.1909241676330566\n",
      "Batch: 50, Loss: 2.167595624923706\n",
      "Batch: 51, Loss: 2.170773506164551\n",
      "Batch: 52, Loss: 2.176833391189575\n",
      "Batch: 53, Loss: 2.1565167903900146\n",
      "Batch: 54, Loss: 2.119532823562622\n",
      "Batch: 55, Loss: 2.1230432987213135\n",
      "Batch: 56, Loss: 2.1416871547698975\n",
      "Batch: 57, Loss: 2.100715398788452\n",
      "Batch: 58, Loss: 2.1146626472473145\n",
      "Batch: 59, Loss: 2.1192877292633057\n",
      "Batch: 60, Loss: 2.1436209678649902\n",
      "Batch: 61, Loss: 2.120922327041626\n",
      "Batch: 62, Loss: 2.0875020027160645\n",
      "Batch: 63, Loss: 2.087580442428589\n",
      "Batch: 64, Loss: 2.104800224304199\n",
      "Batch: 65, Loss: 2.1346566677093506\n",
      "Batch: 66, Loss: 2.105095624923706\n",
      "Batch: 67, Loss: 2.0743091106414795\n",
      "Batch: 68, Loss: 2.1175568103790283\n",
      "Batch: 69, Loss: 2.0749971866607666\n",
      "Batch: 70, Loss: 2.040395498275757\n",
      "Batch: 71, Loss: 2.0589356422424316\n",
      "Batch: 72, Loss: 2.0627198219299316\n",
      "Batch: 73, Loss: 2.062371015548706\n",
      "Batch: 74, Loss: 2.0711536407470703\n",
      "Batch: 75, Loss: 2.0451242923736572\n",
      "Batch: 76, Loss: 1.9929347038269043\n",
      "Batch: 77, Loss: 2.093439817428589\n",
      "Batch: 78, Loss: 2.0415005683898926\n",
      "Batch: 79, Loss: 1.9999783039093018\n",
      "Batch: 80, Loss: 2.0731618404388428\n",
      "Batch: 81, Loss: 2.0099408626556396\n",
      "Batch: 82, Loss: 1.966227650642395\n",
      "Batch: 83, Loss: 1.996374249458313\n",
      "Batch: 84, Loss: 1.9500380754470825\n",
      "Batch: 85, Loss: 2.032473564147949\n",
      "Batch: 86, Loss: 2.0362255573272705\n",
      "Batch: 87, Loss: 1.9684059619903564\n",
      "Batch: 88, Loss: 1.947343111038208\n",
      "Batch: 89, Loss: 2.020293951034546\n",
      "Batch: 90, Loss: 1.937074065208435\n",
      "Batch: 91, Loss: 2.0110342502593994\n",
      "Batch: 92, Loss: 1.9739344120025635\n",
      "Batch: 93, Loss: 1.9326591491699219\n",
      "Batch: 94, Loss: 1.9520378112792969\n",
      "Batch: 95, Loss: 1.9489665031433105\n",
      "Batch: 96, Loss: 1.9084887504577637\n",
      "Batch: 97, Loss: 1.9703878164291382\n",
      "Batch: 98, Loss: 1.9823907613754272\n",
      "Batch: 99, Loss: 1.9808343648910522\n",
      "Batch: 100, Loss: 2.0078608989715576\n",
      "Batch: 101, Loss: 1.9658935070037842\n",
      "Batch: 102, Loss: 1.9703660011291504\n",
      "Batch: 103, Loss: 1.9045119285583496\n",
      "Batch: 104, Loss: 1.9508436918258667\n",
      "Batch: 105, Loss: 1.9133565425872803\n",
      "Batch: 106, Loss: 1.8932511806488037\n",
      "Batch: 107, Loss: 1.8733527660369873\n",
      "Batch: 108, Loss: 1.8558356761932373\n",
      "Batch: 109, Loss: 1.8342618942260742\n",
      "Batch: 110, Loss: 1.8201647996902466\n",
      "Batch: 111, Loss: 1.883374571800232\n",
      "Batch: 112, Loss: 1.8173478841781616\n",
      "Batch: 113, Loss: 1.8530877828598022\n",
      "Batch: 114, Loss: 1.7657444477081299\n",
      "Batch: 115, Loss: 1.8477519750595093\n",
      "Batch: 116, Loss: 1.71091628074646\n",
      "Batch: 117, Loss: 1.9003044366836548\n",
      "Batch: 118, Loss: 1.787163257598877\n",
      "Batch: 119, Loss: 1.838006854057312\n",
      "Batch: 120, Loss: 1.7534090280532837\n",
      "Batch: 121, Loss: 1.9201208353042603\n",
      "Batch: 122, Loss: 1.8262724876403809\n",
      "Batch: 123, Loss: 1.7914384603500366\n",
      "Batch: 124, Loss: 1.7844767570495605\n",
      "Batch: 125, Loss: 1.811193585395813\n",
      "Batch: 126, Loss: 1.7456810474395752\n",
      "Batch: 127, Loss: 1.7881202697753906\n",
      "Batch: 128, Loss: 1.7524603605270386\n",
      "Batch: 129, Loss: 1.8272676467895508\n",
      "Batch: 130, Loss: 1.7048660516738892\n",
      "Batch: 131, Loss: 1.6778658628463745\n",
      "Batch: 132, Loss: 1.714766263961792\n",
      "Batch: 133, Loss: 1.722551703453064\n",
      "Batch: 134, Loss: 1.6499258279800415\n",
      "Batch: 135, Loss: 1.662566065788269\n",
      "Batch: 136, Loss: 1.7684403657913208\n",
      "Batch: 137, Loss: 1.7792322635650635\n",
      "Batch: 138, Loss: 1.6747994422912598\n",
      "Batch: 139, Loss: 1.649593472480774\n",
      "Batch: 140, Loss: 1.6461374759674072\n",
      "Batch: 141, Loss: 1.790581226348877\n",
      "Batch: 142, Loss: 1.6514962911605835\n",
      "Batch: 143, Loss: 1.6357618570327759\n",
      "Batch: 144, Loss: 1.6526944637298584\n",
      "Batch: 145, Loss: 1.6028043031692505\n",
      "Batch: 146, Loss: 1.5982978343963623\n",
      "Batch: 147, Loss: 1.661468267440796\n",
      "Batch: 148, Loss: 1.633899450302124\n",
      "Batch: 149, Loss: 1.6505759954452515\n",
      "Batch: 150, Loss: 1.6519851684570312\n",
      "Batch: 151, Loss: 1.5536835193634033\n",
      "Batch: 152, Loss: 1.6731452941894531\n",
      "Batch: 153, Loss: 1.5891188383102417\n",
      "Batch: 154, Loss: 1.6247167587280273\n",
      "Batch: 155, Loss: 1.6288292407989502\n",
      "Batch: 156, Loss: 1.5493693351745605\n",
      "Batch: 157, Loss: 1.6293457746505737\n",
      "Batch: 158, Loss: 1.646155595779419\n",
      "Batch: 159, Loss: 1.7075823545455933\n",
      "Batch: 160, Loss: 1.5601855516433716\n",
      "Batch: 161, Loss: 1.6359788179397583\n",
      "Batch: 162, Loss: 1.6543328762054443\n",
      "Batch: 163, Loss: 1.5898027420043945\n",
      "Batch: 164, Loss: 1.5378395318984985\n",
      "Batch: 165, Loss: 1.5736500024795532\n",
      "Batch: 166, Loss: 1.5079909563064575\n",
      "Batch: 167, Loss: 1.4420034885406494\n",
      "Batch: 168, Loss: 1.5722631216049194\n",
      "Batch: 169, Loss: 1.6032257080078125\n",
      "Batch: 170, Loss: 1.5143022537231445\n",
      "Batch: 171, Loss: 1.5212252140045166\n",
      "Batch: 172, Loss: 1.4533815383911133\n",
      "Batch: 173, Loss: 1.6026698350906372\n",
      "Batch: 174, Loss: 1.4409538507461548\n",
      "Batch: 175, Loss: 1.41445791721344\n",
      "Batch: 176, Loss: 1.4812142848968506\n",
      "Batch: 177, Loss: 1.5983363389968872\n",
      "Batch: 178, Loss: 1.3538992404937744\n",
      "Batch: 179, Loss: 1.4719979763031006\n",
      "Batch: 180, Loss: 1.3759145736694336\n",
      "Batch: 181, Loss: 1.5867687463760376\n",
      "Batch: 182, Loss: 1.5142546892166138\n",
      "Batch: 183, Loss: 1.4200968742370605\n",
      "Batch: 184, Loss: 1.3368122577667236\n",
      "Batch: 185, Loss: 1.4060966968536377\n",
      "Batch: 186, Loss: 1.3488173484802246\n",
      "Batch: 187, Loss: 1.3889533281326294\n",
      "Batch: 188, Loss: 1.4449392557144165\n",
      "Batch: 189, Loss: 1.4007610082626343\n",
      "Batch: 190, Loss: 1.3080071210861206\n",
      "Batch: 191, Loss: 1.4354182481765747\n",
      "Batch: 192, Loss: 1.412835717201233\n",
      "Batch: 193, Loss: 1.4986945390701294\n",
      "Batch: 194, Loss: 1.4947782754898071\n",
      "Batch: 195, Loss: 1.3933794498443604\n",
      "Batch: 196, Loss: 1.3159403800964355\n",
      "Batch: 197, Loss: 1.2502892017364502\n",
      "Batch: 198, Loss: 1.4704383611679077\n",
      "Batch: 199, Loss: 1.339613437652588\n",
      "Batch: 200, Loss: 1.3669652938842773\n",
      "Batch: 201, Loss: 1.408268690109253\n",
      "Batch: 202, Loss: 1.3766928911209106\n",
      "Batch: 203, Loss: 1.3584290742874146\n",
      "Batch: 204, Loss: 1.4084861278533936\n",
      "Batch: 205, Loss: 1.3592095375061035\n",
      "Batch: 206, Loss: 1.3072936534881592\n",
      "Batch: 207, Loss: 1.3820505142211914\n",
      "Batch: 208, Loss: 1.3444006443023682\n",
      "Batch: 209, Loss: 1.235806941986084\n",
      "Batch: 210, Loss: 1.3160265684127808\n",
      "Batch: 211, Loss: 1.2498250007629395\n",
      "Batch: 212, Loss: 1.4003181457519531\n",
      "Batch: 213, Loss: 1.2763175964355469\n",
      "Batch: 214, Loss: 1.3032809495925903\n",
      "Batch: 215, Loss: 1.1196180582046509\n",
      "Batch: 216, Loss: 1.342040777206421\n",
      "Batch: 217, Loss: 1.3200953006744385\n",
      "Batch: 218, Loss: 1.4522000551223755\n",
      "Batch: 219, Loss: 1.3721051216125488\n",
      "Batch: 220, Loss: 1.2343941926956177\n",
      "Batch: 221, Loss: 1.3099039793014526\n",
      "Batch: 222, Loss: 1.2885193824768066\n",
      "Batch: 223, Loss: 1.267830491065979\n",
      "Batch: 224, Loss: 1.2900925874710083\n",
      "Batch: 225, Loss: 1.2122211456298828\n",
      "Batch: 226, Loss: 1.2932581901550293\n",
      "Batch: 227, Loss: 1.2226699590682983\n",
      "Batch: 228, Loss: 1.281243920326233\n",
      "Batch: 229, Loss: 1.3463259935379028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 230, Loss: 1.2443780899047852\n",
      "Batch: 231, Loss: 1.2140682935714722\n",
      "Batch: 232, Loss: 1.3427790403366089\n",
      "Batch: 233, Loss: 1.115907907485962\n",
      "Batch: 234, Loss: 1.2040200233459473\n",
      "Batch: 235, Loss: 1.4018675088882446\n",
      "Batch: 236, Loss: 1.3054977655410767\n",
      "Batch: 237, Loss: 1.1856940984725952\n",
      "Batch: 238, Loss: 1.2555208206176758\n",
      "Batch: 239, Loss: 1.1556650400161743\n",
      "Batch: 240, Loss: 1.1539603471755981\n",
      "Batch: 241, Loss: 1.1964763402938843\n",
      "Batch: 242, Loss: 1.1661659479141235\n",
      "Batch: 243, Loss: 1.3382189273834229\n",
      "Batch: 244, Loss: 1.0965142250061035\n",
      "Batch: 245, Loss: 1.173755168914795\n",
      "Batch: 246, Loss: 1.282906174659729\n",
      "Batch: 247, Loss: 1.1114957332611084\n",
      "Batch: 248, Loss: 1.2254294157028198\n",
      "Batch: 249, Loss: 1.1858657598495483\n",
      "Batch: 250, Loss: 1.1011098623275757\n",
      "Batch: 251, Loss: 1.23004949092865\n",
      "Batch: 252, Loss: 1.1434880495071411\n",
      "Batch: 253, Loss: 1.0658663511276245\n",
      "Batch: 254, Loss: 1.0187327861785889\n",
      "Batch: 255, Loss: 1.2080317735671997\n",
      "Batch: 256, Loss: 1.1579567193984985\n",
      "Batch: 257, Loss: 1.2267879247665405\n",
      "Batch: 258, Loss: 1.1264475584030151\n",
      "Batch: 259, Loss: 1.093930721282959\n",
      "Batch: 260, Loss: 1.1958645582199097\n",
      "Batch: 261, Loss: 1.254819393157959\n",
      "Batch: 262, Loss: 1.1431788206100464\n",
      "Batch: 263, Loss: 1.0960232019424438\n",
      "Batch: 264, Loss: 1.0613385438919067\n",
      "Batch: 265, Loss: 1.1455409526824951\n",
      "Batch: 266, Loss: 1.1083053350448608\n",
      "Batch: 267, Loss: 1.1469074487686157\n",
      "Batch: 268, Loss: 1.0699876546859741\n",
      "Batch: 269, Loss: 1.1545144319534302\n",
      "Batch: 270, Loss: 1.1117987632751465\n",
      "Batch: 271, Loss: 1.1068302392959595\n",
      "Batch: 272, Loss: 1.0274955034255981\n",
      "Batch: 273, Loss: 1.0516313314437866\n",
      "Batch: 274, Loss: 1.1068713665008545\n",
      "Batch: 275, Loss: 0.9517826437950134\n",
      "Batch: 276, Loss: 1.0973780155181885\n",
      "Batch: 277, Loss: 1.0850956439971924\n",
      "Batch: 278, Loss: 1.0541387796401978\n",
      "Batch: 279, Loss: 1.050610899925232\n",
      "Batch: 280, Loss: 1.221909523010254\n",
      "Batch: 281, Loss: 1.0500788688659668\n",
      "Batch: 282, Loss: 1.1538500785827637\n",
      "Batch: 283, Loss: 1.003629446029663\n",
      "Batch: 284, Loss: 1.0411652326583862\n",
      "Batch: 285, Loss: 1.0567150115966797\n",
      "Batch: 286, Loss: 1.1080623865127563\n",
      "Batch: 287, Loss: 0.9144642353057861\n",
      "Batch: 288, Loss: 1.1202350854873657\n",
      "Batch: 289, Loss: 1.110558032989502\n",
      "Batch: 290, Loss: 1.1209615468978882\n",
      "Batch: 291, Loss: 1.0900479555130005\n",
      "Batch: 292, Loss: 0.9630472660064697\n",
      "Batch: 293, Loss: 1.0489885807037354\n",
      "Batch: 294, Loss: 1.0792567729949951\n",
      "Batch: 295, Loss: 1.0308568477630615\n",
      "Batch: 296, Loss: 1.0164963006973267\n",
      "Batch: 297, Loss: 1.024631142616272\n",
      "Batch: 298, Loss: 1.0618395805358887\n",
      "Batch: 299, Loss: 0.9712465405464172\n",
      "Batch: 300, Loss: 1.1577438116073608\n",
      "Batch: 301, Loss: 0.9831181764602661\n",
      "Batch: 302, Loss: 0.9848618507385254\n",
      "Batch: 303, Loss: 0.9847302436828613\n",
      "Batch: 304, Loss: 0.8740223050117493\n",
      "Batch: 305, Loss: 0.9627156257629395\n",
      "Batch: 306, Loss: 1.0101865530014038\n",
      "Batch: 307, Loss: 1.0947006940841675\n",
      "Batch: 308, Loss: 0.963843047618866\n",
      "Batch: 309, Loss: 0.9141924381256104\n",
      "Batch: 310, Loss: 1.0287573337554932\n",
      "Batch: 311, Loss: 1.1636847257614136\n",
      "Batch: 312, Loss: 0.8538344502449036\n",
      "Batch: 313, Loss: 0.9889305830001831\n",
      "Batch: 314, Loss: 1.035810947418213\n",
      "Batch: 315, Loss: 0.9372119307518005\n",
      "Batch: 316, Loss: 0.9494583010673523\n",
      "Batch: 317, Loss: 1.045782208442688\n",
      "Batch: 318, Loss: 0.8595943450927734\n",
      "Batch: 319, Loss: 1.0612674951553345\n",
      "Batch: 320, Loss: 1.0196110010147095\n",
      "Batch: 321, Loss: 0.8770536780357361\n",
      "Batch: 322, Loss: 0.95933997631073\n",
      "Batch: 323, Loss: 0.9212438464164734\n",
      "Batch: 324, Loss: 0.8432738780975342\n",
      "Batch: 325, Loss: 0.8965447545051575\n",
      "Batch: 326, Loss: 0.9513490200042725\n",
      "Batch: 327, Loss: 1.0094588994979858\n",
      "Batch: 328, Loss: 0.9549077749252319\n",
      "Batch: 329, Loss: 0.8125509023666382\n",
      "Batch: 330, Loss: 0.9265959858894348\n",
      "Batch: 331, Loss: 1.026210904121399\n",
      "Batch: 332, Loss: 1.02415132522583\n",
      "Batch: 333, Loss: 0.9229016900062561\n",
      "Batch: 334, Loss: 1.0192469358444214\n",
      "Batch: 335, Loss: 0.970287561416626\n",
      "Batch: 336, Loss: 0.8675970435142517\n",
      "Batch: 337, Loss: 0.8122144937515259\n",
      "Batch: 338, Loss: 0.9958395957946777\n",
      "Batch: 339, Loss: 0.8138613104820251\n",
      "Batch: 340, Loss: 0.9475015997886658\n",
      "Batch: 341, Loss: 0.990312397480011\n",
      "Batch: 342, Loss: 0.9694657325744629\n",
      "Batch: 343, Loss: 0.8020480871200562\n",
      "Batch: 344, Loss: 0.8098591566085815\n",
      "Batch: 345, Loss: 0.8375105261802673\n",
      "Batch: 346, Loss: 0.8742523789405823\n",
      "Batch: 347, Loss: 0.9727158546447754\n",
      "Batch: 348, Loss: 1.0206319093704224\n",
      "Batch: 349, Loss: 0.9736632108688354\n",
      "Batch: 350, Loss: 0.8547430038452148\n",
      "Batch: 351, Loss: 0.9206287860870361\n",
      "Batch: 352, Loss: 0.8893576860427856\n",
      "Batch: 353, Loss: 0.8894689083099365\n",
      "Batch: 354, Loss: 0.8381764888763428\n",
      "Batch: 355, Loss: 0.9566296935081482\n",
      "Batch: 356, Loss: 0.9755065441131592\n",
      "Batch: 357, Loss: 0.8092015385627747\n",
      "Batch: 358, Loss: 0.8831230401992798\n",
      "Batch: 359, Loss: 0.9761728048324585\n",
      "Batch: 360, Loss: 0.8527743816375732\n",
      "Batch: 361, Loss: 0.9204023480415344\n",
      "Batch: 362, Loss: 0.9223178029060364\n",
      "Batch: 363, Loss: 0.8850990533828735\n",
      "Batch: 364, Loss: 0.8838069438934326\n",
      "Batch: 365, Loss: 0.9067901968955994\n",
      "Batch: 366, Loss: 1.0579485893249512\n",
      "Batch: 367, Loss: 0.8031981587409973\n",
      "Batch: 368, Loss: 0.8904291987419128\n",
      "Batch: 369, Loss: 0.8885176777839661\n",
      "Batch: 370, Loss: 0.884740948677063\n",
      "Batch: 371, Loss: 0.9490715265274048\n",
      "Batch: 372, Loss: 0.8310813903808594\n",
      "Batch: 373, Loss: 0.8137192130088806\n",
      "Batch: 374, Loss: 0.8131090998649597\n",
      "Batch: 375, Loss: 0.7660781741142273\n",
      "Batch: 376, Loss: 0.8644000887870789\n",
      "Batch: 377, Loss: 0.9422706365585327\n",
      "Batch: 378, Loss: 0.8855081796646118\n",
      "Batch: 379, Loss: 0.7673274278640747\n",
      "Batch: 380, Loss: 0.8133147358894348\n",
      "Batch: 381, Loss: 0.9320031404495239\n",
      "Batch: 382, Loss: 0.8649585843086243\n",
      "Batch: 383, Loss: 0.9236421585083008\n",
      "Batch: 384, Loss: 0.7722771167755127\n",
      "Batch: 385, Loss: 0.8975470066070557\n",
      "Batch: 386, Loss: 0.887823760509491\n",
      "Batch: 387, Loss: 0.5921804904937744\n",
      "Batch: 388, Loss: 0.8909934759140015\n",
      "Batch: 389, Loss: 0.894393801689148\n",
      "Batch: 390, Loss: 0.9825973510742188\n",
      "Batch: 391, Loss: 0.8629485964775085\n",
      "Batch: 392, Loss: 0.8722583055496216\n",
      "Batch: 393, Loss: 0.9258859753608704\n",
      "Batch: 394, Loss: 0.9161372780799866\n",
      "Batch: 395, Loss: 0.9599263668060303\n",
      "Batch: 396, Loss: 0.7576858997344971\n",
      "Batch: 397, Loss: 0.8301110863685608\n",
      "Batch: 398, Loss: 0.7295325994491577\n",
      "Batch: 399, Loss: 0.9730861783027649\n",
      "Batch: 400, Loss: 0.9307123422622681\n",
      "Batch: 401, Loss: 0.7890152931213379\n",
      "Batch: 402, Loss: 0.8173393607139587\n",
      "Batch: 403, Loss: 0.8856228590011597\n",
      "Batch: 404, Loss: 0.7846047878265381\n",
      "Batch: 405, Loss: 0.7618324756622314\n",
      "Batch: 406, Loss: 0.92579185962677\n",
      "Batch: 407, Loss: 0.8842496871948242\n",
      "Batch: 408, Loss: 0.8112576603889465\n",
      "Batch: 409, Loss: 0.9705411791801453\n",
      "Batch: 410, Loss: 0.7815613746643066\n",
      "Batch: 411, Loss: 0.7933449745178223\n",
      "Batch: 412, Loss: 0.8094362020492554\n",
      "Batch: 413, Loss: 0.9492307305335999\n",
      "Batch: 414, Loss: 0.9741339683532715\n",
      "Batch: 415, Loss: 0.920537531375885\n",
      "Batch: 416, Loss: 0.8660681247711182\n",
      "Batch: 417, Loss: 0.9039760231971741\n",
      "Batch: 418, Loss: 0.7970015406608582\n",
      "Batch: 419, Loss: 0.823434591293335\n",
      "Batch: 420, Loss: 0.9064974188804626\n",
      "Batch: 421, Loss: 0.7526271343231201\n",
      "Batch: 422, Loss: 1.0503320693969727\n",
      "Batch: 423, Loss: 0.7594854235649109\n",
      "Batch: 424, Loss: 0.6812637448310852\n",
      "Batch: 425, Loss: 0.9555196762084961\n",
      "Batch: 426, Loss: 0.7376278042793274\n",
      "Batch: 427, Loss: 0.8431446552276611\n",
      "Batch: 428, Loss: 0.8815688490867615\n",
      "Batch: 429, Loss: 0.8358431458473206\n",
      "Batch: 430, Loss: 0.8098349571228027\n",
      "Batch: 431, Loss: 0.7581434845924377\n",
      "Batch: 432, Loss: 0.6801068782806396\n",
      "Batch: 433, Loss: 0.8866687417030334\n",
      "Batch: 434, Loss: 0.7102667093276978\n",
      "Batch: 435, Loss: 0.8478506207466125\n",
      "Batch: 436, Loss: 0.8805927038192749\n",
      "Batch: 437, Loss: 0.9708710312843323\n",
      "Batch: 438, Loss: 0.853836715221405\n",
      "Batch: 439, Loss: 0.8771015405654907\n",
      "Batch: 440, Loss: 0.8507152199745178\n",
      "Batch: 441, Loss: 0.8652411103248596\n",
      "Batch: 442, Loss: 0.7155756950378418\n",
      "Batch: 443, Loss: 0.8596692681312561\n",
      "Batch: 444, Loss: 0.724256694316864\n",
      "Batch: 445, Loss: 0.886315643787384\n",
      "Batch: 446, Loss: 0.7182594537734985\n",
      "Batch: 447, Loss: 0.8924359679222107\n",
      "Batch: 448, Loss: 0.8969932198524475\n",
      "Batch: 449, Loss: 0.8984818458557129\n",
      "Batch: 450, Loss: 0.8505533933639526\n",
      "Batch: 451, Loss: 0.963521420955658\n",
      "Batch: 452, Loss: 1.0614919662475586\n",
      "Batch: 453, Loss: 0.778373658657074\n",
      "Batch: 454, Loss: 0.8335104584693909\n",
      "Batch: 455, Loss: 0.9947054982185364\n",
      "Batch: 456, Loss: 0.7210046052932739\n",
      "Batch: 457, Loss: 0.736402690410614\n",
      "Batch: 458, Loss: 0.7309615015983582\n",
      "Batch: 459, Loss: 0.7162827849388123\n",
      "Batch: 460, Loss: 0.6828447580337524\n",
      "Batch: 461, Loss: 0.7996336817741394\n",
      "Batch: 462, Loss: 0.8469101190567017\n",
      "Batch: 463, Loss: 1.019700288772583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 464, Loss: 0.81292724609375\n",
      "Batch: 465, Loss: 0.8566833734512329\n",
      "Batch: 466, Loss: 0.7959210872650146\n",
      "Batch: 467, Loss: 0.8799583315849304\n",
      "Batch: 468, Loss: 0.8677557706832886\n",
      "Batch: 469, Loss: 0.8151605129241943\n",
      "Batch: 470, Loss: 0.7727620005607605\n",
      "Batch: 471, Loss: 0.7839208841323853\n",
      "Batch: 472, Loss: 0.7476712465286255\n",
      "Batch: 473, Loss: 0.8931441307067871\n",
      "Batch: 474, Loss: 0.7824521660804749\n",
      "Batch: 475, Loss: 0.7838314175605774\n",
      "Batch: 476, Loss: 0.7332865595817566\n",
      "Batch: 477, Loss: 0.7414096593856812\n",
      "Batch: 478, Loss: 0.7301003336906433\n",
      "Batch: 479, Loss: 0.864790141582489\n",
      "Batch: 480, Loss: 0.7664608359336853\n",
      "Batch: 481, Loss: 0.6764508485794067\n",
      "Batch: 482, Loss: 0.826641857624054\n",
      "Batch: 483, Loss: 0.815302848815918\n",
      "Batch: 484, Loss: 0.8251277208328247\n",
      "Batch: 485, Loss: 0.6247996687889099\n",
      "Batch: 486, Loss: 0.7449228167533875\n",
      "Batch: 487, Loss: 0.7727482914924622\n",
      "Batch: 488, Loss: 0.6419260501861572\n",
      "Batch: 489, Loss: 0.8319789171218872\n",
      "Batch: 490, Loss: 0.7450084686279297\n",
      "Batch: 491, Loss: 0.5867926478385925\n",
      "Batch: 492, Loss: 0.6815366148948669\n",
      "Batch: 493, Loss: 0.7501041889190674\n",
      "Batch: 494, Loss: 0.6335068941116333\n",
      "Batch: 495, Loss: 0.9039780497550964\n",
      "Batch: 496, Loss: 0.9407376646995544\n",
      "Batch: 497, Loss: 0.783053457736969\n",
      "Batch: 498, Loss: 0.7702822685241699\n",
      "Batch: 499, Loss: 0.795421302318573\n",
      "Batch: 500, Loss: 0.828656792640686\n",
      "Batch: 501, Loss: 0.6875041127204895\n",
      "Batch: 502, Loss: 0.672936201095581\n",
      "Batch: 503, Loss: 0.7376566529273987\n",
      "Batch: 504, Loss: 0.788336455821991\n",
      "Batch: 505, Loss: 0.8771886825561523\n",
      "Batch: 506, Loss: 0.7339561581611633\n",
      "Batch: 507, Loss: 0.6748456954956055\n",
      "Batch: 508, Loss: 0.9704158306121826\n",
      "Batch: 509, Loss: 0.7333958745002747\n",
      "Batch: 510, Loss: 0.8187533617019653\n",
      "Batch: 511, Loss: 0.8454981446266174\n",
      "Batch: 512, Loss: 0.7328206300735474\n",
      "Batch: 513, Loss: 0.9156756401062012\n",
      "Batch: 514, Loss: 0.7030953168869019\n",
      "Batch: 515, Loss: 0.9119811058044434\n",
      "Batch: 516, Loss: 0.8237978219985962\n",
      "Batch: 517, Loss: 0.8064717054367065\n",
      "Batch: 518, Loss: 0.7936767339706421\n",
      "Batch: 519, Loss: 0.6412559747695923\n",
      "Batch: 520, Loss: 0.7320554256439209\n",
      "Batch: 521, Loss: 0.581606388092041\n",
      "Batch: 522, Loss: 0.7681925296783447\n",
      "Batch: 523, Loss: 0.861971378326416\n",
      "Batch: 524, Loss: 0.8951334357261658\n",
      "Batch: 525, Loss: 0.8034395575523376\n",
      "Batch: 526, Loss: 0.6608409881591797\n",
      "Batch: 527, Loss: 0.7896682620048523\n",
      "Batch: 528, Loss: 0.5994271636009216\n",
      "Batch: 529, Loss: 0.7104864120483398\n",
      "Batch: 530, Loss: 0.8121788501739502\n",
      "Batch: 531, Loss: 0.8122903108596802\n",
      "Batch: 532, Loss: 0.6990257501602173\n",
      "Batch: 533, Loss: 0.6482719779014587\n",
      "Batch: 534, Loss: 0.6982793807983398\n",
      "Batch: 535, Loss: 0.6977294087409973\n",
      "Batch: 536, Loss: 0.6070196032524109\n",
      "Batch: 537, Loss: 0.7699666619300842\n",
      "Batch: 538, Loss: 0.7679358124732971\n",
      "Batch: 539, Loss: 0.5719769597053528\n",
      "Batch: 540, Loss: 0.6565212607383728\n",
      "Batch: 541, Loss: 0.6275808811187744\n",
      "Batch: 542, Loss: 0.6773709058761597\n",
      "Batch: 543, Loss: 0.792672872543335\n",
      "Batch: 544, Loss: 0.702235758304596\n",
      "Batch: 545, Loss: 0.855490505695343\n",
      "Batch: 546, Loss: 0.8044986724853516\n",
      "Batch: 547, Loss: 0.8353395462036133\n",
      "Batch: 548, Loss: 0.7810187339782715\n",
      "Batch: 549, Loss: 0.7163552641868591\n",
      "Batch: 550, Loss: 0.6067243814468384\n",
      "Batch: 551, Loss: 0.8146591782569885\n",
      "Batch: 552, Loss: 0.6828365325927734\n",
      "Batch: 553, Loss: 0.6910255551338196\n",
      "Batch: 554, Loss: 0.6246902942657471\n",
      "Batch: 555, Loss: 0.6801997423171997\n",
      "Batch: 556, Loss: 0.5987305641174316\n",
      "Batch: 557, Loss: 0.6237297058105469\n",
      "Batch: 558, Loss: 0.8075142502784729\n",
      "Batch: 559, Loss: 0.7598640322685242\n",
      "Batch: 560, Loss: 0.7310506105422974\n",
      "Batch: 561, Loss: 0.6259545683860779\n",
      "Batch: 562, Loss: 0.5659353137016296\n",
      "Batch: 563, Loss: 0.8234437108039856\n",
      "Batch: 564, Loss: 0.7148224711418152\n",
      "Batch: 565, Loss: 0.7079187035560608\n",
      "Batch: 566, Loss: 0.7056533098220825\n",
      "Batch: 567, Loss: 0.7099376916885376\n",
      "Batch: 568, Loss: 0.6773277521133423\n",
      "Batch: 569, Loss: 0.7676053047180176\n",
      "Batch: 570, Loss: 0.6196939945220947\n",
      "Batch: 571, Loss: 0.786159336566925\n",
      "Batch: 572, Loss: 0.6208330988883972\n",
      "Batch: 573, Loss: 0.7778885960578918\n",
      "Batch: 574, Loss: 0.6763424873352051\n",
      "Batch: 575, Loss: 0.7680953741073608\n",
      "Batch: 576, Loss: 0.7287394404411316\n",
      "Batch: 577, Loss: 0.6801403760910034\n",
      "Batch: 578, Loss: 0.7436360120773315\n",
      "Batch: 579, Loss: 0.7413076758384705\n",
      "Batch: 580, Loss: 0.6562257409095764\n",
      "Batch: 581, Loss: 0.8694367408752441\n",
      "Batch: 582, Loss: 0.5909163951873779\n",
      "Batch: 583, Loss: 0.6813505291938782\n",
      "Batch: 584, Loss: 0.746516227722168\n",
      "Batch: 585, Loss: 0.6455032825469971\n",
      "Batch: 586, Loss: 0.6227273941040039\n",
      "Batch: 587, Loss: 0.6165330410003662\n",
      "Batch: 588, Loss: 0.8517565131187439\n",
      "Batch: 589, Loss: 0.7406730055809021\n",
      "Batch: 590, Loss: 0.6439404487609863\n",
      "Batch: 591, Loss: 0.8325140476226807\n",
      "Batch: 592, Loss: 0.6787757873535156\n",
      "Batch: 593, Loss: 0.7139797210693359\n",
      "Batch: 594, Loss: 0.793912410736084\n",
      "Batch: 595, Loss: 1.0826146602630615\n",
      "Batch: 596, Loss: 0.8261110186576843\n",
      "Batch: 597, Loss: 0.6746485233306885\n",
      "Batch: 598, Loss: 0.7994933724403381\n",
      "Batch: 599, Loss: 0.7539932727813721\n",
      "Batch: 600, Loss: 0.6809886693954468\n",
      "Batch: 601, Loss: 0.5973630547523499\n",
      "Batch: 602, Loss: 0.6500412225723267\n",
      "Batch: 603, Loss: 0.853005051612854\n",
      "Batch: 604, Loss: 0.7372008562088013\n",
      "Batch: 605, Loss: 0.7204368114471436\n",
      "Batch: 606, Loss: 0.9100412726402283\n",
      "Batch: 607, Loss: 0.5812628865242004\n",
      "Batch: 608, Loss: 0.7281342148780823\n",
      "Batch: 609, Loss: 0.9157391786575317\n",
      "Batch: 610, Loss: 0.7491303086280823\n",
      "Batch: 611, Loss: 0.6063948273658752\n",
      "Batch: 612, Loss: 0.6295719146728516\n",
      "Batch: 613, Loss: 0.5826238989830017\n",
      "Batch: 614, Loss: 0.9954565167427063\n",
      "Batch: 615, Loss: 0.7363443970680237\n",
      "Batch: 616, Loss: 0.6387535929679871\n",
      "Batch: 617, Loss: 0.7151786684989929\n",
      "Batch: 618, Loss: 0.6519577503204346\n",
      "Batch: 619, Loss: 0.6059598326683044\n",
      "Batch: 620, Loss: 0.823944628238678\n",
      "Batch: 621, Loss: 0.7041513323783875\n",
      "Batch: 622, Loss: 0.7517611384391785\n",
      "Batch: 623, Loss: 0.783406138420105\n",
      "Batch: 624, Loss: 0.6568653583526611\n",
      "Batch: 625, Loss: 0.8303267359733582\n",
      "Batch: 626, Loss: 0.5953472256660461\n",
      "Batch: 627, Loss: 0.7506955862045288\n",
      "Batch: 628, Loss: 0.7589181065559387\n",
      "Batch: 629, Loss: 0.8066167831420898\n",
      "Batch: 630, Loss: 0.7289696335792542\n",
      "Batch: 631, Loss: 0.7028409838676453\n",
      "Batch: 632, Loss: 0.5818377733230591\n",
      "Batch: 633, Loss: 0.7136272192001343\n",
      "Batch: 634, Loss: 0.6642634868621826\n",
      "Batch: 635, Loss: 0.660398542881012\n",
      "Batch: 636, Loss: 0.6658997535705566\n",
      "Batch: 637, Loss: 0.7241374254226685\n",
      "Batch: 638, Loss: 0.7420950531959534\n",
      "Batch: 639, Loss: 0.8062565326690674\n",
      "Batch: 640, Loss: 0.7051796913146973\n",
      "Batch: 641, Loss: 0.5880041718482971\n",
      "Batch: 642, Loss: 0.597252607345581\n",
      "Batch: 643, Loss: 0.7716403007507324\n",
      "Batch: 644, Loss: 0.7747750282287598\n",
      "Batch: 645, Loss: 0.588919460773468\n",
      "Batch: 646, Loss: 0.7427269816398621\n",
      "Batch: 647, Loss: 0.7788745164871216\n",
      "Batch: 648, Loss: 0.7138078212738037\n",
      "Batch: 649, Loss: 0.794084906578064\n",
      "Batch: 650, Loss: 0.6026303768157959\n",
      "Batch: 651, Loss: 0.6958102583885193\n",
      "Batch: 652, Loss: 0.5492726564407349\n",
      "Batch: 653, Loss: 0.789811909198761\n",
      "Batch: 654, Loss: 0.4946352541446686\n",
      "Batch: 655, Loss: 0.5723245739936829\n",
      "Batch: 656, Loss: 0.6144689321517944\n",
      "Batch: 657, Loss: 0.5622974634170532\n",
      "Batch: 658, Loss: 0.6601784825325012\n",
      "Batch: 659, Loss: 0.6564247608184814\n",
      "Batch: 660, Loss: 0.6380419731140137\n",
      "Batch: 661, Loss: 0.7640615105628967\n",
      "Batch: 662, Loss: 0.5447423458099365\n",
      "Batch: 663, Loss: 0.8048092126846313\n",
      "Batch: 664, Loss: 0.7843205332756042\n",
      "Batch: 665, Loss: 0.6503046154975891\n",
      "Batch: 666, Loss: 0.7309140563011169\n",
      "Batch: 667, Loss: 0.640937328338623\n",
      "Batch: 668, Loss: 0.725009560585022\n",
      "Batch: 669, Loss: 0.6777904629707336\n",
      "Batch: 670, Loss: 0.6231788992881775\n",
      "Batch: 671, Loss: 0.5729856491088867\n",
      "Batch: 672, Loss: 0.817357063293457\n",
      "Batch: 673, Loss: 0.6930325031280518\n",
      "Batch: 674, Loss: 0.7172963619232178\n",
      "Batch: 675, Loss: 0.9464015960693359\n",
      "Batch: 676, Loss: 0.7621055245399475\n",
      "Batch: 677, Loss: 0.7005460262298584\n",
      "Batch: 678, Loss: 0.7305516600608826\n",
      "Batch: 679, Loss: 0.6690582036972046\n",
      "Batch: 680, Loss: 0.8534792065620422\n",
      "Batch: 681, Loss: 0.6198579668998718\n",
      "Batch: 682, Loss: 0.5953599214553833\n",
      "Batch: 683, Loss: 0.6397819519042969\n",
      "Batch: 684, Loss: 0.6284002065658569\n",
      "Batch: 685, Loss: 0.6421725153923035\n",
      "Batch: 686, Loss: 0.7030207514762878\n",
      "Batch: 687, Loss: 0.8566731810569763\n",
      "Batch: 688, Loss: 0.7487530708312988\n",
      "Batch: 689, Loss: 0.6380375623703003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 690, Loss: 0.7255526781082153\n",
      "Batch: 691, Loss: 0.7232828140258789\n",
      "Batch: 692, Loss: 0.722409188747406\n",
      "Batch: 693, Loss: 0.7636272311210632\n",
      "Batch: 694, Loss: 0.7034024000167847\n",
      "Batch: 695, Loss: 0.7502437829971313\n",
      "Batch: 696, Loss: 0.8505018353462219\n",
      "Batch: 697, Loss: 0.6978367567062378\n",
      "Batch: 698, Loss: 0.7250263094902039\n",
      "Batch: 699, Loss: 0.8582481741905212\n",
      "Batch: 700, Loss: 0.8512320518493652\n",
      "Batch: 701, Loss: 0.6327056884765625\n",
      "Batch: 702, Loss: 0.6065342426300049\n",
      "Batch: 703, Loss: 0.6422102451324463\n",
      "Batch: 704, Loss: 0.6038904786109924\n",
      "Batch: 705, Loss: 0.6565707921981812\n",
      "Batch: 706, Loss: 0.6937658786773682\n",
      "Batch: 707, Loss: 0.6600814461708069\n",
      "Batch: 708, Loss: 0.7112292051315308\n",
      "Batch: 709, Loss: 0.7098615765571594\n",
      "Batch: 710, Loss: 0.5268065929412842\n",
      "Batch: 711, Loss: 0.6000372171401978\n",
      "Batch: 712, Loss: 0.5753609538078308\n",
      "Batch: 713, Loss: 0.687133252620697\n",
      "Batch: 714, Loss: 0.8461347818374634\n",
      "Batch: 715, Loss: 0.8451234102249146\n",
      "Batch: 716, Loss: 0.5139820575714111\n",
      "Batch: 717, Loss: 0.6774318218231201\n",
      "Batch: 718, Loss: 0.8603097796440125\n",
      "Batch: 719, Loss: 0.6650294065475464\n",
      "Batch: 720, Loss: 0.5893168449401855\n",
      "Batch: 721, Loss: 0.5107816457748413\n",
      "Batch: 722, Loss: 0.7209864854812622\n",
      "Batch: 723, Loss: 0.7578026056289673\n",
      "Batch: 724, Loss: 0.5715532898902893\n",
      "Batch: 725, Loss: 0.6343413591384888\n",
      "Batch: 726, Loss: 0.7381987571716309\n",
      "Batch: 727, Loss: 0.6379216313362122\n",
      "Batch: 728, Loss: 0.4675561785697937\n",
      "Batch: 729, Loss: 0.5611637830734253\n",
      "Batch: 730, Loss: 0.7167226672172546\n",
      "Batch: 731, Loss: 0.5636007189750671\n",
      "Batch: 732, Loss: 0.5867354869842529\n",
      "Batch: 733, Loss: 0.5885063409805298\n",
      "Batch: 734, Loss: 0.6150491237640381\n",
      "Batch: 735, Loss: 0.7212913632392883\n",
      "Batch: 736, Loss: 0.4309902489185333\n",
      "Batch: 737, Loss: 0.648414134979248\n",
      "Batch: 738, Loss: 0.6372358798980713\n",
      "Batch: 739, Loss: 0.6757640242576599\n",
      "Batch: 740, Loss: 0.7488603591918945\n",
      "Batch: 741, Loss: 0.6898088455200195\n",
      "Batch: 742, Loss: 0.6025667190551758\n",
      "Batch: 743, Loss: 0.6624886989593506\n",
      "Batch: 744, Loss: 0.6828656196594238\n",
      "Batch: 745, Loss: 0.5264497995376587\n",
      "Batch: 746, Loss: 0.7911113500595093\n",
      "Batch: 747, Loss: 0.5684614777565002\n",
      "Batch: 748, Loss: 0.7620279788970947\n",
      "Batch: 749, Loss: 0.6306798458099365\n",
      "Batch: 750, Loss: 0.7075662016868591\n",
      "Batch: 751, Loss: 0.7740783095359802\n",
      "Batch: 752, Loss: 0.5576790571212769\n",
      "Batch: 753, Loss: 0.5311409831047058\n",
      "Batch: 754, Loss: 0.6219182014465332\n",
      "Batch: 755, Loss: 0.8577633500099182\n",
      "Batch: 756, Loss: 0.966284453868866\n",
      "Batch: 757, Loss: 0.6414380669593811\n",
      "Batch: 758, Loss: 0.6791516542434692\n",
      "Batch: 759, Loss: 0.6280646324157715\n",
      "Batch: 760, Loss: 0.6839918494224548\n",
      "Batch: 761, Loss: 0.785983681678772\n",
      "Batch: 762, Loss: 0.5712488293647766\n",
      "Batch: 763, Loss: 0.5666601061820984\n",
      "Batch: 764, Loss: 0.7858725190162659\n",
      "Batch: 765, Loss: 0.6183145642280579\n",
      "Batch: 766, Loss: 0.6539730429649353\n",
      "Batch: 767, Loss: 0.5407705307006836\n",
      "Batch: 768, Loss: 0.5331252813339233\n",
      "Batch: 769, Loss: 0.6551975607872009\n",
      "Batch: 770, Loss: 0.7182667255401611\n",
      "Batch: 771, Loss: 0.6026812791824341\n",
      "Batch: 772, Loss: 0.6185612678527832\n",
      "Batch: 773, Loss: 0.6852439045906067\n",
      "Batch: 774, Loss: 0.779954195022583\n",
      "Batch: 775, Loss: 0.794041633605957\n",
      "Batch: 776, Loss: 0.5412623286247253\n",
      "Batch: 777, Loss: 0.6224229335784912\n",
      "Batch: 778, Loss: 0.6343780755996704\n",
      "Batch: 779, Loss: 0.7858176827430725\n",
      "Batch: 780, Loss: 0.5848875641822815\n",
      "Batch: 781, Loss: 0.6281525492668152\n",
      "Batch: 782, Loss: 0.6541953086853027\n",
      "Batch: 783, Loss: 0.4942731261253357\n",
      "Batch: 784, Loss: 0.6027260422706604\n",
      "Batch: 785, Loss: 0.4380849599838257\n",
      "Batch: 786, Loss: 0.7119436264038086\n",
      "Batch: 787, Loss: 0.6328887343406677\n",
      "Batch: 788, Loss: 0.7231086492538452\n",
      "Batch: 789, Loss: 0.5687467455863953\n",
      "Batch: 790, Loss: 0.6730384826660156\n",
      "Batch: 791, Loss: 0.7408555150032043\n",
      "Batch: 792, Loss: 0.6557329297065735\n",
      "Batch: 793, Loss: 0.5620589852333069\n",
      "Batch: 794, Loss: 0.699074387550354\n",
      "Batch: 795, Loss: 0.7397933602333069\n",
      "Batch: 796, Loss: 0.7857369780540466\n",
      "Batch: 797, Loss: 0.4591994881629944\n",
      "Batch: 798, Loss: 0.7965134382247925\n",
      "Batch: 799, Loss: 0.8505100011825562\n",
      "Batch: 800, Loss: 0.6444627642631531\n",
      "Batch: 801, Loss: 0.7757440209388733\n",
      "Batch: 802, Loss: 0.6063584089279175\n",
      "Batch: 803, Loss: 0.565613329410553\n",
      "Batch: 804, Loss: 0.5907281041145325\n",
      "Batch: 805, Loss: 0.820392906665802\n",
      "Batch: 806, Loss: 0.7539815306663513\n",
      "Batch: 807, Loss: 0.7166545987129211\n",
      "Batch: 808, Loss: 0.7379440665245056\n",
      "Batch: 809, Loss: 0.6201005578041077\n",
      "Batch: 810, Loss: 0.652134358882904\n",
      "Batch: 811, Loss: 0.6449950933456421\n",
      "Batch: 812, Loss: 0.7428930401802063\n",
      "Batch: 813, Loss: 0.8248971700668335\n",
      "Batch: 814, Loss: 0.5601897239685059\n",
      "Batch: 815, Loss: 0.615099310874939\n",
      "Batch: 816, Loss: 0.6041055917739868\n",
      "Batch: 817, Loss: 0.4881647825241089\n",
      "Batch: 818, Loss: 0.6281734704971313\n",
      "Batch: 819, Loss: 0.49494174122810364\n",
      "Batch: 820, Loss: 0.6521756649017334\n",
      "Batch: 821, Loss: 0.6419171690940857\n",
      "Batch: 822, Loss: 0.5988435745239258\n",
      "Batch: 823, Loss: 0.8148878216743469\n",
      "Batch: 824, Loss: 0.6400526165962219\n",
      "Batch: 825, Loss: 0.7517958283424377\n",
      "Batch: 826, Loss: 0.5084136724472046\n",
      "Batch: 827, Loss: 0.7202180027961731\n",
      "Batch: 828, Loss: 0.5171824097633362\n",
      "Batch: 829, Loss: 0.815104603767395\n",
      "Batch: 830, Loss: 0.7826434969902039\n",
      "Batch: 831, Loss: 0.6649947166442871\n",
      "Batch: 832, Loss: 0.5785633325576782\n",
      "Batch: 833, Loss: 0.5157937407493591\n",
      "Batch: 834, Loss: 0.5649396181106567\n",
      "Batch: 835, Loss: 0.7199205160140991\n",
      "Batch: 836, Loss: 0.6904016137123108\n",
      "Batch: 837, Loss: 0.5897279977798462\n",
      "Batch: 838, Loss: 0.5503016710281372\n",
      "Batch: 839, Loss: 0.5124139785766602\n",
      "Batch: 840, Loss: 0.6040745973587036\n",
      "Batch: 841, Loss: 0.7670683264732361\n",
      "Batch: 842, Loss: 0.7695255875587463\n",
      "Batch: 843, Loss: 0.49633166193962097\n",
      "Batch: 844, Loss: 0.5648353099822998\n",
      "Batch: 845, Loss: 0.5140982866287231\n",
      "Batch: 846, Loss: 0.5417423844337463\n",
      "Batch: 847, Loss: 0.9007467031478882\n",
      "Batch: 848, Loss: 0.9657776355743408\n",
      "Batch: 849, Loss: 0.6200476884841919\n",
      "Batch: 850, Loss: 0.5529205799102783\n",
      "Batch: 851, Loss: 0.7188102602958679\n",
      "Batch: 852, Loss: 0.797650933265686\n",
      "Batch: 853, Loss: 0.6577052474021912\n",
      "Batch: 854, Loss: 0.5464089512825012\n",
      "Batch: 855, Loss: 0.5963422656059265\n",
      "Batch: 856, Loss: 0.5517163276672363\n",
      "Batch: 857, Loss: 0.5644544959068298\n",
      "Batch: 858, Loss: 0.47294774651527405\n",
      "Batch: 859, Loss: 0.5971715450286865\n",
      "Batch: 860, Loss: 0.6971424221992493\n",
      "Batch: 861, Loss: 0.5674349665641785\n",
      "Batch: 862, Loss: 0.688173770904541\n",
      "Batch: 863, Loss: 0.5960215926170349\n",
      "Batch: 864, Loss: 0.5713467597961426\n",
      "Batch: 865, Loss: 0.8029208183288574\n",
      "Batch: 866, Loss: 0.7214202284812927\n",
      "Batch: 867, Loss: 0.6991333365440369\n",
      "Batch: 868, Loss: 0.6198926568031311\n",
      "Batch: 869, Loss: 0.45495134592056274\n",
      "Batch: 870, Loss: 0.6951215267181396\n",
      "Batch: 871, Loss: 0.7800943851470947\n",
      "Batch: 872, Loss: 0.6003513336181641\n",
      "Batch: 873, Loss: 0.7451679110527039\n",
      "Batch: 874, Loss: 1.0854864120483398\n",
      "Batch: 875, Loss: 0.8187114596366882\n",
      "Batch: 876, Loss: 0.6941676139831543\n",
      "Batch: 877, Loss: 0.6241883039474487\n",
      "Batch: 878, Loss: 0.7336876392364502\n",
      "Batch: 879, Loss: 0.6854028701782227\n",
      "Batch: 880, Loss: 0.6761919856071472\n",
      "Batch: 881, Loss: 0.4673735201358795\n",
      "Batch: 882, Loss: 0.6528622508049011\n",
      "Batch: 883, Loss: 0.5023443698883057\n",
      "Batch: 884, Loss: 0.6580853462219238\n",
      "Batch: 885, Loss: 0.726447582244873\n",
      "Batch: 886, Loss: 0.7784879207611084\n",
      "Batch: 887, Loss: 0.4823155999183655\n",
      "Batch: 888, Loss: 0.5711461901664734\n",
      "Batch: 889, Loss: 0.6398043632507324\n",
      "Batch: 890, Loss: 0.49704709649086\n",
      "Batch: 891, Loss: 0.5884112119674683\n",
      "Batch: 892, Loss: 0.4844506084918976\n",
      "Batch: 893, Loss: 0.8091756105422974\n",
      "Batch: 894, Loss: 0.5772874355316162\n",
      "Batch: 895, Loss: 0.6824678182601929\n",
      "Batch: 896, Loss: 0.9438543915748596\n",
      "Batch: 897, Loss: 0.5400363206863403\n",
      "Batch: 898, Loss: 0.7408564686775208\n",
      "Batch: 899, Loss: 0.5619300007820129\n",
      "Batch: 900, Loss: 0.4685659110546112\n",
      "Batch: 901, Loss: 0.5866252183914185\n",
      "Batch: 902, Loss: 0.5675826072692871\n",
      "Batch: 903, Loss: 0.6790246963500977\n",
      "Batch: 904, Loss: 0.5486919283866882\n",
      "Batch: 905, Loss: 0.7346237897872925\n",
      "Batch: 906, Loss: 0.6039757132530212\n",
      "Batch: 907, Loss: 0.6890202760696411\n",
      "Batch: 908, Loss: 0.7185475826263428\n",
      "Batch: 909, Loss: 0.49065569043159485\n",
      "Batch: 910, Loss: 0.48489174246788025\n",
      "Batch: 911, Loss: 0.6424047946929932\n",
      "Batch: 912, Loss: 0.5369430780410767\n",
      "Batch: 913, Loss: 0.46390411257743835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 914, Loss: 0.6200928688049316\n",
      "Batch: 915, Loss: 0.7076222896575928\n",
      "Batch: 916, Loss: 0.6444756984710693\n",
      "Batch: 917, Loss: 0.7690625190734863\n",
      "Batch: 918, Loss: 0.6698958873748779\n",
      "Batch: 919, Loss: 0.827814519405365\n",
      "Batch: 920, Loss: 0.728588342666626\n",
      "Batch: 921, Loss: 0.5826833248138428\n",
      "Batch: 922, Loss: 0.5327664017677307\n",
      "Batch: 923, Loss: 0.6287207007408142\n",
      "Batch: 924, Loss: 0.5486977100372314\n",
      "Batch: 925, Loss: 0.7654973864555359\n",
      "Batch: 926, Loss: 0.5996663570404053\n",
      "Batch: 927, Loss: 0.6182467341423035\n",
      "Batch: 928, Loss: 0.6924936175346375\n",
      "Batch: 929, Loss: 0.4430564045906067\n",
      "Batch: 930, Loss: 0.7466434836387634\n",
      "Batch: 931, Loss: 0.5913838148117065\n",
      "Batch: 932, Loss: 0.5907655358314514\n",
      "Batch: 933, Loss: 0.6467644572257996\n",
      "Batch: 934, Loss: 0.5608280897140503\n",
      "Batch: 935, Loss: 0.4923921823501587\n",
      "Batch: 936, Loss: 0.6768632531166077\n",
      "Batch: 937, Loss: 0.4640061855316162\n",
      "Batch: 938, Loss: 0.5052637457847595\n",
      "Training loss: 1.0306918401199618\n"
     ]
    }
   ],
   "source": [
    "model = FMNIST()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    cum_loss = 0\n",
    "    batch_num = 0\n",
    "    \n",
    "    for batch_num,(images, labels) in enumerate(trainloader,1):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        cum_loss += loss.item()\n",
    "        print(f'Batch: {batch_num}, Loss: {loss.item()}')\n",
    "     \n",
    "    print(f\"Training loss: {cum_loss/len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-OVDFUnzFGpr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "937.5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "60000/64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hWf1SWuiFGmn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OQ_QUMXLFGjr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "03_04_Using_Optimizers.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
